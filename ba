sudo apt-get install apt-transport-https curl gnupg -y
curl -fsSL https://bazel.build/bazel-release.pub.gpg | gpg --dearmor >bazel-archive-keyring.gpg
sudo mv bazel-archive-keyring.gpg /usr/share/keyrings/bazel-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/bazel-archive-keyring.gpg] https://storage.googleapis.com/bazel-apt stable jdk1.8" | sudo tee /etc/apt/sources.list.d/bazel.list
sudo apt-get update && sudo apt-get install bazel

bazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/face_detection:face_detection_cpu
bazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/python:build_pip_package

mkdir -p mediapipe/dist
python3 setup.py bdist_wheel
sudo pip3 install mediapipe/dist/*.whl

import mediapipe as mp
mp_face_detection = mp.solutions.face_detection.FaceDetection()

import cv2
import numpy as np
import tensorflow as tf
import mediapipe as mp
import time
import RPi.GPIO as GPIO

# Setup GPIO
GPIO.setmode(GPIO.BCM)
GPIO.setup(17, GPIO.OUT)  # Relay pin
GPIO.setup(27, GPIO.OUT)  # Buzzer pin

# Function to activate relay
def activate_relay():
    GPIO.output(17, GPIO.HIGH)  # Aktifkan relay
    time.sleep(2)  # Durasi aktif
    GPIO.output(17, GPIO.LOW)   # Matikan relay

# Function to activate buzzer
def activate_buzzer():
    GPIO.output(27, GPIO.HIGH)  # Aktifkan buzzer
    time.sleep(2)  # Durasi buzzer aktif
    GPIO.output(27, GPIO.LOW)   # Matikan buzzer

# Load the TFLite model and allocate tensors
def load_tflite_model(model_path):
    interpreter = tf.lite.Interpreter(model_path=model_path)
    interpreter.allocate_tensors()
    return interpreter

# Preprocess the image for TFLite model input
def preprocess_image(image):
    image = cv2.resize(image, (224, 224))
    image = image.astype(np.float32)
    image = np.expand_dims(image, axis=0)
    return image

# Run inference with TFLite model
def inference(interpreter, image):
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    interpreter.set_tensor(input_details[0]['index'], image)
    interpreter.invoke()
    output_data = interpreter.get_tensor(output_details[0]['index'])
    return output_data

def main():
    # Initialize MediaPipe Face Detection
    mp_face_detection = mp.solutions.face_detection
    mp_drawing = mp.solutions.drawing_utils

    # Load TFLite model
    model_path = '/home/pi/raspberry_proyek/data/models/face_recognition_model.tflite'
    interpreter = load_tflite_model(model_path)

    # Setup video capture
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)  # Set frame width
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)  # Set frame height

    # Add an initial delay before starting detection
    print("Waiting for 5 seconds...")
    time.sleep(5)  # Delay selama 5 detik

    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            # Convert the frame to RGB
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # Process the frame for face detection
            results = face_detection.process(rgb_frame)

            if results.detections:
                for detection in results.detections:
                    # Draw the face detection annotations on the frame
                    mp_drawing.draw_detection(frame, detection)

                    # Extract the bounding box
                    bboxC = detection.location_data.relative_bounding_box
                    ih, iw, _ = frame.shape
                    (x, y, w, h) = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \
                                   int(bboxC.width * iw), int(bboxC.height * ih)
                    
                    # Crop the face from the frame
                    face = frame[y:y+h, x:x+w]

                    # Process 3 frames for more stable prediction
                    labels = []
                    for _ in range(7):
                        # Preprocess the face for the TFLite model
                        face_input = preprocess_image(face)

                        # Run inference
                        output = inference(interpreter, face_input)
                        
                        # Assuming the model output is a probability distribution, get the highest score
                        label = np.argmax(output)
                        labels.append(label)
                    
                    # Get the most common label from the 3 frames
                    final_label = max(set(labels), key=labels.count)
                    confidence_score = output[0][final_label]
                    label_text = f"User: {confidence_score:.2f}" if final_label == 1 else f"Maling: {confidence_score:.2f}"
                    cv2.putText(frame, label_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

                    # Activate relay or buzzer based on final_label
                    if final_label == 1:
                        activate_relay()  # Jika wajah dikenali
                    else:
                        activate_buzzer()  # Jika wajah tidak dikenali

            # Show the frame with the face detection and recognition results
            cv2.imshow('Face Detection and Recognition', frame)

            # Press 'q' to quit the loop
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    # Release resources
    cap.release()
    cv2.destroyAllWindows()
    GPIO.cleanup()  # Clean up GPIO

if __name__ == '__main__':
    main()
